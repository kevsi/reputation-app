# Sentinelle Reputations - Scrapers Service

Service de collecte de donnÃ©es pour Sentinelle Reputation. Ce service expose une API REST FastAPI pour dÃ©clencher des scrapings sur diverses sources (Google Reviews, Trustpilot, News, etc.) et intÃ¨gre Ã©galement des spiders Scrapy pour des tÃ¢ches de fond plus lourdes ou complexes.

## ğŸ¯ FonctionnalitÃ©s

- **API REST** : FastAPI pour une interaction facile et standardisÃ©e.
- **Support Multi-Sources** : Google Reviews, Trustpilot, TripAdvisor, News API, Twitter, Reddit, RSS, YouTube.
- **Robustesse** : Rate limiting, gestion d'erreurs, retry logic.
- **Architecture Hybride** : API synchrone/asynchrone + Spiders Scrapy (fallback).

## ğŸ“ Structure

```
scrapers/
â”œâ”€â”€ api/                              # API FastAPI
â”‚   â”œâ”€â”€ main.py                       # Point d'entrÃ©e
â”‚   â”œâ”€â”€ config.py                     # Configuration
â”‚   â”œâ”€â”€ routes/                       # Endpoints par source
â”‚   â”œâ”€â”€ models/                       # ModÃ¨les Pydantic ({Request, Response} schemas)
â”‚   â””â”€â”€ utils/                        # Rate limiter, validateurs, logger
â”œâ”€â”€ sentinelle_scrapers/              # Scrapers Scrapy (Legacy/Advanced)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ .env                              # Configuration (clÃ©s API)
```

## ğŸš€ Installation & DÃ©marrage

### PrÃ©-requis

- Python 3.9+
- ClÃ©s API pour les services externes (Google, NewsAPI, Twitter, etc.)

### Installation Locale

1. CrÃ©er un environnement virtuel :
   ```bash
   python -m venv venv
   source venv/bin/activate  # Ou venv\Scripts\activate sur Windows
   ```

2. Installer les dÃ©pendances :
   ```bash
   pip install -r requirements.txt
   ```

3. Configurer l'environnement :
   Copier `.env.example` vers `.env` et remplir les variables.
   ```bash
   cp .env.example .env
   ```

4. Lancer le serveur :
   ```bash
   python -m api.main
   # Ou
   uvicorn api.main:app --reload
   ```

   L'API sera accessible sur `http://localhost:8001`.
   Documentation Swagger : `http://localhost:8001/docs`

### Docker

```bash
docker build -t sentinelle-scrapers .
docker run -p 8001:8001 --env-file .env sentinelle-scrapers
```

## ğŸ”Œ Endpoints Principaux

- `POST /scrape/google-reviews` : Scraper les avis Google Maps.
- `POST /scrape/trustpilot` : Scraper les avis Trustpilot.
- `POST /scrape/tripadvisor` : Scraper les avis TripAdvisor.
- `POST /scrape/news` : Scraper les articles de presse via NewsAPI.
- `POST /scrape/twitter` : Scraper Twitter (X).
- `POST /scrape/reddit` : Scraper Reddit.
- `POST /scrape/rss` : Scraper un flux RSS.
- `POST /scrape/youtube` : Scraper les commentaires/vidÃ©os YouTube.

Voir `/docs` pour les dÃ©tails des payloads (paramÃ¨tres, clÃ©s API, etc.).

## ğŸ›¡ï¸ Rate Limiting

L'API utilise `slowapi` pour limiter le nombre de requÃªtes par IP afin de protÃ©ger les ressources et les quotas API externes.
Configuration par dÃ©faut : `RATE_LIMIT_PER_MINUTE=60` (ajustable dans `.env`).

## ğŸ› ï¸ DÃ©veloppement

### Tests
```bash
pytest api/tests/ -v
```

### Ajouter une nouvelle source
1. CrÃ©er le modÃ¨le de requÃªte dans `api/models/schemas.py`.
2. CrÃ©er le fichier de route `api/routes/nouvelle_source.py`.
3. ImplÃ©menter la logique de scraping (API externe ou HTML scraping).
4. Enregistrer la route dans `api/main.py`.
