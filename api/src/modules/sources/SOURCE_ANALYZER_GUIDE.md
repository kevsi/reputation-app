/**
 * üìö SourceAnalyzer - Documentation & Exemples d'Utilisation
 * 
 * Guide complet du module d'analyse des sources
 */

// ============================================================================
// 1. INSTALLATION & CONFIGURATION
// ============================================================================

/**
 * Installation des d√©pendances requises:
 * 
 * npm install axios p-retry
 */

// ============================================================================
// 2. INT√âGRATION AVEC EXPRESS
// ============================================================================

/**
 * Exemple d'int√©gration dans src/modules/sources/index.ts:
 * 
 * import { Router } from 'express';
 * import { PrismaClient } from '@sentinelle/database';
 * import { Logger } from 'winston';
 * import createSourceAnalyzerRoutes from './source-analyzer.routes';
 * 
 * export function createSourceRoutes(
 *   prisma: PrismaClient,
 *   logger: Logger
 * ): Router {
 *   const router = Router();
 *   
 *   // Inclure les routes d'analyse
 *   router.use('/analyze', createSourceAnalyzerRoutes(prisma, logger));
 *   
 *   // Autres routes de sources...
 *   
 *   return router;
 * }
 */

// ============================================================================
// 3. UTILISATION DIRECTE DU MODULE
// ============================================================================

/**
 * Exemple 1: Analyse simple d'une URL
 * 
 * import SourceAnalyzer from './source-analyzer';
 * 
 * async function example1() {
 *   const analyzer = new SourceAnalyzer({
 *     timeout: 10000,
 *     maxRetries: 2
 *   });
 * 
 *   const diagnostic = await analyzer.analyze('https://news.example.com');
 * 
 *   console.log('Strat√©gie:', diagnostic.strategy);
 *   console.log('Message:', diagnostic.message);
 *   console.log('Recommandations:', diagnostic.recommendations);
 * 
 *   // Acc√©der aux logs pour debugging
 *   console.log('Logs:', diagnostic.logs);
 * }
 */

// ============================================================================
// 4. UTILISATION VIA LE SERVICE
// ============================================================================

/**
 * Exemple 2: Utiliser le service avec Prisma
 * 
 * import { PrismaClient } from '@sentinelle/database';
 * import { createLogger } from 'winston';
 * import SourceAnalyzerService from './source-analyzer.service';
 * 
 * async function example2() {
 *   const prisma = new PrismaClient();
 *   const logger = createLogger({...});
 * 
 *   const service = new SourceAnalyzerService(prisma, logger);
 * 
 *   // Analyser une URL
 *   const result = await service.analyzeUrl('https://example.com/blog');
 * 
 *   console.log('Strat√©gie d√©tect√©e:', result.diagnostic.strategy);
 *   console.log('Type de source sugg√©r√©:', result.suggestedSourceType);
 *   console.log('Config sugg√©r√©e:', result.suggestedConfig);
 * 
 *   // Analyser et cr√©er automatiquement une source
 *   if (!result.requiresUserAction) {
 *     const source = await service.createSourceFromDiagnostic(
 *       result.diagnostic,
 *       brandId,
 *       organizationId,
 *       'Mon blog'
 *     );
 *     console.log('Source cr√©√©e:', source?.id);
 *   }
 * }
 */

// ============================================================================
// 5. APPELS API HTTP
// ============================================================================

/**
 * Exemple 3: Analyser une URL via HTTP (curl)
 * 
 * curl -X POST http://localhost:5001/api/sources/analyze \
 *   -H "Content-Type: application/json" \
 *   -d '{
 *     "url": "https://techblog.example.com",
 *     "includeDebugLogs": false
 *   }'
 * 
 * R√©ponse:
 * {
 *   "success": true,
 *   "data": {
 *     "diagnostic": {
 *       "url": "https://techblog.example.com",
 *       "strategy": "SCRAPABLE",
 *       "status": 200,
 *       "hasContent": true,
 *       "isJavaScriptOnly": false,
 *       "blockageType": "NONE",
 *       "message": "Cette source peut √™tre scrapp√©e directement...",
 *       "recommendations": [
 *         "La source sera scrapp√©e toutes les 6 heures par d√©faut",
 *         "Vous pouvez ajuster la fr√©quence de scraping..."
 *       ],
 *       "logs": [...]
 *     },
 *     "suggestedSourceType": "BLOG",
 *     "suggestedConfig": {
 *       "url": "https://techblog.example.com",
 *       "scrapingFrequency": 21600,
 *       "method": "cheerio"
 *     },
 *     "requiresUserAction": false
 *   },
 *   "timestamp": "2026-01-28T10:30:00.000Z"
 * }
 */

// ============================================================================
// 6. ANALYSE BATCH
// ============================================================================

/**
 * Exemple 4: Analyser plusieurs URLs √† la fois
 * 
 * const urls = [
 *   'https://blog1.example.com',
 *   'https://protected.example.com',
 *   'https://forum.example.com',
 *   'https://invalid-url'
 * ];
 * 
 * curl -X POST http://localhost:5001/api/sources/analyze/batch \
 *   -H "Content-Type: application/json" \
 *   -d "{\"urls\": $(echo $urls | jq -R 'split(\" \")')}"
 * 
 * R√©ponse:
 * {
 *   "success": true,
 *   "data": {
 *     "total": 4,
 *     "results": [
 *       {
 *         "diagnostic": { "strategy": "SCRAPABLE", ... },
 *         "requiresUserAction": false
 *       },
 *       {
 *         "diagnostic": { "strategy": "GOOGLE_SEARCH", ... },
 *         "requiresUserAction": false
 *       },
 *       ...
 *     ]
 *   }
 * }
 */

// ============================================================================
// 7. ANALYSER ET CR√âER UNE SOURCE
// ============================================================================

/**
 * Exemple 5: Analyser et cr√©er automatiquement une source
 * 
 * curl -X POST http://localhost:5001/api/sources/analyze-and-create \
 *   -H "Content-Type: application/json" \
 *   -d '{
 *     "url": "https://techblog.example.com",
 *     "brandId": "550e8400-e29b-41d4-a716-446655440000",
 *     "name": "TechBlog",
 *     "organizationId": "550e8400-e29b-41d4-a716-446655440001"
 *   }'
 * 
 * R√©ponse:
 * {
 *   "success": true,
 *   "data": {
 *     "analysis": { ... },
 *     "source": {
 *       "id": "...",
 *       "name": "TechBlog",
 *       "type": "BLOG",
 *       "url": "https://techblog.example.com",
 *       "scrapingFrequency": 21600,
 *       "config": {
 *         "url": "...",
 *         "analysisMeta": {
 *           "blockageDetected": "NONE",
 *           "isJavaScriptOnly": false,
 *           "analyzedAt": "2026-01-28T10:30:00.000Z"
 *         }
 *       }
 *     },
 *     "created": true
 *   }
 * }
 */

// ============================================================================
// 8. INTERPR√âTATION DES STRAT√âGIES
// ============================================================================

/**
 * SCRAPABLE: Source scrappable directement
 * ‚îú‚îÄ Propri√©t√©s:
 * ‚îÇ  ‚îú‚îÄ HTML valide disponible
 * ‚îÇ  ‚îú‚îÄ Pas de blocage (Cloudflare, WAF, etc.)
 * ‚îÇ  ‚îú‚îÄ robots.txt permet le scraping
 * ‚îÇ  ‚îî‚îÄ Pas JS-only
 * ‚îî‚îÄ Actions:
 *    ‚îú‚îÄ Scraper avec Cheerio/Playwright
 *    ‚îú‚îÄ Fr√©quence: 6 heures (par d√©faut)
 *    ‚îî‚îÄ Processeur: scraping.processor
 * 
 * GOOGLE_SEARCH: Passer par Google Search API
 * ‚îú‚îÄ Propri√©t√©s:
 * ‚îÇ  ‚îú‚îÄ Source bloqu√©e (Cloudflare, reCAPTCHA, WAF)
 * ‚îÇ  ‚îú‚îÄ robots.txt refuse le scraping
 * ‚îÇ  ‚îú‚îÄ JavaScript-only
 * ‚îÇ  ‚îî‚îÄ Pas de contenu directement accessible
 * ‚îî‚îÄ Actions:
 *    ‚îú‚îÄ Utiliser Google Search API
 *    ‚îú‚îÄ Chercher: brand + keywords
 *    ‚îî‚îÄ Fr√©quence: 24 heures (API limit)
 * 
 * API_REQUIRED: N√©cessite une cl√© API
 * ‚îú‚îÄ Propri√©t√©s:
 * ‚îÇ  ‚îú‚îÄ Content-Type: application/json
 * ‚îÇ  ‚îú‚îÄ Endpoint d'API d√©tect√©
 * ‚îÇ  ‚îî‚îÄ Authentification requise
 * ‚îî‚îÄ Actions:
 *    ‚îú‚îÄ Demander la cl√© API utilisateur
 *    ‚îú‚îÄ Valider les credentials
 *    ‚îî‚îÄ Utiliser le collector API sp√©cifique
 * 
 * UNSUPPORTED: Non support√©
 * ‚îú‚îÄ Propri√©t√©s:
 * ‚îÇ  ‚îú‚îÄ URL invalide
 * ‚îÇ  ‚îú‚îÄ Ressource non trouv√©e (404)
 * ‚îÇ  ‚îú‚îÄ Serveur inaccessible
 * ‚îÇ  ‚îî‚îÄ Contenu vide
 * ‚îî‚îÄ Actions:
 *    ‚îú‚îÄ Afficher erreur √† l'utilisateur
 *    ‚îú‚îÄ Sugg√©rer alternatives
 *    ‚îî‚îÄ Ne pas cr√©er de source
 */

// ============================================================================
// 9. D√âTECTION DES BLOCAGES
// ============================================================================

/**
 * Blocages d√©tect√©s automatiquement:
 * 
 * CLOUDFLARE
 * ‚îú‚îÄ En-t√™tes HTTP:
 * ‚îÇ  ‚îú‚îÄ server: cloudflare
 * ‚îÇ  ‚îú‚îÄ cf-ray: ...
 * ‚îÇ  ‚îî‚îÄ cf-connecting-ip: ...
 * ‚îú‚îÄ Contenu HTML:
 * ‚îÇ  ‚îî‚îÄ "Cloudflare", "cf_clearance"
 * ‚îî‚îÄ R√©solution: Google Search API
 * 
 * RECAPTCHA
 * ‚îú‚îÄ HTML:
 * ‚îÇ  ‚îú‚îÄ g-recaptcha
 * ‚îÇ  ‚îú‚îÄ hcaptcha
 * ‚îÇ  ‚îî‚îÄ grecaptcha script
 * ‚îî‚îÄ R√©solution: Google Search API
 * 
 * WAF (Web Application Firewall)
 * ‚îú‚îÄ Messages:
 * ‚îÇ  ‚îú‚îÄ "Web Application Firewall"
 * ‚îÇ  ‚îú‚îÄ "Access Denied"
 * ‚îÇ  ‚îî‚îÄ "403 Forbidden"
 * ‚îî‚îÄ R√©solution: Google Search API
 * 
 * JAVASCRIPT-ONLY
 * ‚îú‚îÄ Frameworks d√©tect√©s:
 * ‚îÇ  ‚îú‚îÄ Next.js (__NEXT_DATA__)
 * ‚îÇ  ‚îú‚îÄ Nuxt (__NUXT__)
 * ‚îÇ  ‚îú‚îÄ React (React...)
 * ‚îÇ  ‚îú‚îÄ Vue (data-v-app)
 * ‚îÇ  ‚îî‚îÄ Angular (ng-app)
 * ‚îî‚îÄ R√©solution: Google Search API ou Playwright
 */

// ============================================================================
// 10. ROBOTS.TXT
// ============================================================================

/**
 * V√©rification automatique du robots.txt:
 * 
 * ‚úÖ Permet le scraping:
 * User-agent: *
 * Disallow: /admin
 * Disallow: /private
 * 
 * ‚ùå Refuse le scraping:
 * User-agent: *
 * Disallow: /
 * 
 * ‚úÖ Pas de robots.txt:
 * ‚Üí Scraping autoris√© par d√©faut
 */

// ============================================================================
// 11. LOGGING & DEBUGGING
// ============================================================================

/**
 * Acc√©der aux logs d√©taill√©s:
 * 
 * const diagnostic = await analyzer.analyze('https://example.com');
 * 
 * diagnostic.logs.forEach(log => {
 *   console.log(`[${log.timestamp.toISOString()}] ${log.level} - ${log.step}`);
 *   console.log(`  ${log.message}`);
 *   if (log.details) {
 *     console.log('  D√©tails:', log.details);
 *   }
 *   if (log.duration) {
 *     console.log(`  Dur√©e: ${log.duration}ms`);
 *   }
 * });
 * 
 * Logs disponibles:
 * ‚îú‚îÄ INIT: D√©but de l'analyse
 * ‚îú‚îÄ CONNECTION: Test de connexion
 * ‚îú‚îÄ FETCH: R√©cup√©ration du contenu
 * ‚îú‚îÄ ANALYZE_CONTENT: Analyse du HTML
 * ‚îú‚îÄ ROBOTS: V√©rification de robots.txt
 * ‚îú‚îÄ STRATEGY: D√©termination de la strat√©gie
 * ‚îú‚îÄ COMPLETE: Fin de l'analyse
 * ‚îî‚îÄ ERROR: Erreurs rencontr√©es
 */

// ============================================================================
// 12. GESTION DES ERREURS
// ============================================================================

/**
 * Try-catch complet:
 * 
 * try {
 *   const analyzer = new SourceAnalyzer();
 *   const diagnostic = await analyzer.analyze(url);
 * 
 *   switch (diagnostic.strategy) {
 *     case CollectionStrategy.SCRAPABLE:
 *       // Cr√©er source web
 *       break;
 *     case CollectionStrategy.GOOGLE_SEARCH:
 *       // V√©rifier Google Search API
 *       break;
 *     case CollectionStrategy.API_REQUIRED:
 *       // Demander cl√© API
 *       break;
 *     case CollectionStrategy.UNSUPPORTED:
 *       // Afficher erreur
 *       break;
 *   }
 * } catch (error) {
 *   console.error('Erreur lors de l\'analyse:', error);
 *   // Afficher message d'erreur g√©n√©rique
 * }
 */

// ============================================================================
// 13. PERFORMANCE
// ============================================================================

/**
 * Optimisations appliqu√©es:
 * 
 * ‚úÖ Timeout: 10 secondes (configurable)
 * ‚úÖ Retries: 2 tentatives automatiques
 * ‚úÖ HEAD request en premier (plus rapide)
 * ‚úÖ Content-length limit: 5MB
 * ‚úÖ Logs optionnels (includeDebugLogs=false)
 * 
 * Temps moyen par URL:
 * ‚îú‚îÄ Scrapable: 200-500ms
 * ‚îú‚îÄ Bloqu√©e: 300-700ms
 * ‚îú‚îÄ Erreur: 100-200ms
 * ‚îî‚îÄ Batch (10 URLs): 2-5 secondes
 */

// ============================================================================
// 14. EXTENSIBILIT√â
// ============================================================================

/**
 * Ajouter une nouvelle d√©tection de blocage:
 * 
 * // Dans source-analyzer.ts, m√©thode detectBlockage():
 * 
 * private readonly CUSTOM_INDICATORS = ['Custom Blocker'];
 * 
 * if (html.includes('Custom Blocker')) {
 *   return BlockageType.CUSTOM; // Ajouter √† l'enum
 * }
 * 
 * // Ajouter une nouvelle strat√©gie:
 * 
 * enum CollectionStrategy {
 *   CUSTOM_STRATEGY = 'CUSTOM_STRATEGY'
 * }
 * 
 * // Mettre √† jour determineStrategy()
 */

// ============================================================================
// 15. INT√âGRATION AVEC LES WORKERS
// ============================================================================

/**
 * Workflow complet:
 * 
 * 1Ô∏è‚É£ Utilisateur soumet une URL via l'API
 *    POST /api/sources/analyze-and-create
 * 
 * 2Ô∏è‚É£ SourceAnalyzer d√©tecte la strat√©gie
 *    ‚Üí SCRAPABLE: Cr√©e une source de type BLOG/FORUM/etc.
 *    ‚Üí GOOGLE_SEARCH: Cr√©e une source de type NEWS
 *    ‚Üí UNSUPPORTED: Retourne une erreur
 * 
 * 3Ô∏è‚É£ Une source est cr√©√©e dans la BD avec la config
 *    config: {
 *      url: "https://...",
 *      analysisMeta: { blockageDetected, isJavaScriptOnly, ... }
 *    }
 * 
 * 4Ô∏è‚É£ Le worker scraping.processor r√©cup√®re la source
 *    ‚îú‚îÄ Regarde config.analysisMeta
 *    ‚îú‚îÄ Si isJavaScriptOnly: utilise Playwright
 *    ‚îî‚îÄ Sinon: utilise Cheerio
 * 
 * 5Ô∏è‚É£ Le collector utilise la meilleure strat√©gie
 *    ‚îú‚îÄ Si SCRAPABLE: scrape directement
 *    ‚îú‚îÄ Si GOOGLE_SEARCH: utilise Google Search API
 *    ‚îî‚îÄ Si API_REQUIRED: utilise la cl√© API
 * 
 * 6Ô∏è‚É£ Les mentions sont collect√©es et stock√©es
 *    Source.lastScrapedAt = now
 *    Source.errorCount = 0 (si succ√®s)
 */

export {};
