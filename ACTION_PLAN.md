# üöÄ PLAN D'ACTION - SENTINELLE REPUTATION

**Date de cr√©ation:** 7 F√©vrier 2026  
**Objectif:** Corriger les failles critiques et pr√©parer la production  
**Dur√©e estim√©e:** 2-3 semaines

---

## üìä VUE D'ENSEMBLE

### Statistiques du Projet

| M√©trique | Valeur |
|----------|--------|
| **Modules API** | 14 modules |
| **Failles critiques** | 5 probl√®mes |
| **Failles moyennes** | 8 probl√®mes |
| **Am√©liorations propos√©es** | 12 t√¢ches |
| **Effort total estim√©** | 15-20 jours |

### Phase 3 : Optimisations (Performance & Scalabilit√©) ‚úÖ
| ID | T√¢che | Description | Priorit√© | √âtat |
|---|---|---|---|---|
| 3.1 | Caching Redis | Cache des agr√©gations analytics et listes fr√©quentes | Haute | ‚úÖ |
| 3.2 | Index BD | Optimisation des index Postgres pour les gros volumes | Haute | ‚úÖ |
| 3.3 | Archivage | Syst√®me d'archivage automatique vers S3 (vieilles donn√©es) | Moyenne | ‚úÖ |

### R√©partition par Priorit√©

```
üî¥ P0 - CRITIQUE (5 t√¢ches)     ‚Üí 5 jours
üü† P1 - IMPORTANT (4 t√¢ches)    ‚Üí 6 jours
üü° P2 - MOYEN (3 t√¢ches)        ‚Üí 4 jours
üü¢ P3 - OPTIONNEL (3 t√¢ches)    ‚Üí 3 jours
```

---

## üî¥ PHASE 1 : CORRECTIONS CRITIQUES (Semaine 1)

### T√¢che 1.1 : Impl√©menter Workers BullMQ ‚è±Ô∏è 2 jours

**Priorit√©:** üî¥ P0 - CRITIQUE  
**Impact:** Sans workers, le scraping automatique ne fonctionne pas  
**Effort:** 2 jours

#### Probl√®me Actuel
```
‚ùå Dossier api/src/workers/ n'existe pas
‚ùå Pas de scheduler automatique
‚ùå Scraping manuel uniquement via POST /sources/:id/scrape-now
```

#### Solution

**√âtape 1.1.1 : Cr√©er la structure workers/**

```bash
mkdir -p api/src/workers/{processors,jobs,schedulers}
```

**Fichiers √† cr√©er:**

```typescript
// api/src/workers/processors/scraping.processor.ts
import { Worker, Job } from 'bullmq';
import { exec } from 'child_process';
import { promisify } from 'util';
import { prisma } from '@/shared/database/prisma.client';
import { logger } from '@/infrastructure/logger';
import { redisConnection } from '@/config/redis';

const execAsync = promisify(exec);

export const scrapingWorker = new Worker(
  'scraping',
  async (job: Job) => {
    const { sourceId, force } = job.data;
    
    logger.info(`üöÄ Starting scraping job for source ${sourceId}`);
    
    // 1. R√©cup√©rer la source
    const source = await prisma.source.findUnique({
      where: { id: sourceId },
      include: { brand: true }
    });
    
    if (!source) {
      throw new Error(`Source ${sourceId} not found`);
    }
    
    if (!source.isActive && !force) {
      logger.warn(`‚è∏Ô∏è Source ${sourceId} is inactive, skipping`);
      return { skipped: true };
    }
    
    // 2. Cr√©er un scraping job en DB
    const scrapingJob = await prisma.scrapingJob.create({
      data: {
        sourceId,
        status: 'RUNNING',
        startedAt: new Date()
      }
    });
    
    try {
      // 3. Construire la commande Scrapy
      const spiderName = source.type.toLowerCase();
      const config = source.config as any;
      
      let command = `cd scrapers && scrapy crawl ${spiderName}`;
      command += ` -a source_id=${sourceId}`;
      
      // Ajouter les param√®tres selon le type
      if (config.companyName) {
        command += ` -a company_name=${config.companyName}`;
      }
      if (config.url) {
        command += ` -a url=${config.url}`;
      }
      if (source.brand.keywords?.length) {
        command += ` -a keywords=${source.brand.keywords.join(',')}`;
      }
      
      logger.info(`üìù Executing: ${command}`);
      
      // 4. Ex√©cuter le scraper
      const { stdout, stderr } = await execAsync(command, {
        timeout: 30 * 60 * 1000 // 30 minutes max
      });
      
      logger.info(`‚úÖ Scraping completed for source ${sourceId}`);
      logger.debug(`STDOUT: ${stdout}`);
      
      if (stderr) {
        logger.warn(`STDERR: ${stderr}`);
      }
      
      // 5. Compter les mentions cr√©√©es
      const mentionsCount = await prisma.mention.count({
        where: {
          sourceId,
          createdAt: {
            gte: scrapingJob.startedAt
          }
        }
      });
      
      // 6. Mettre √† jour le job
      await prisma.scrapingJob.update({
        where: { id: scrapingJob.id },
        data: {
          status: 'COMPLETED',
          completedAt: new Date(),
          mentionsFound: mentionsCount,
          mentionsCreated: mentionsCount
        }
      });
      
      // 7. Mettre √† jour la source
      await prisma.source.update({
        where: { id: sourceId },
        data: {
          lastScrapedAt: new Date(),
          errorCount: 0,
          lastError: null
        }
      });
      
      return {
        success: true,
        mentionsCreated: mentionsCount,
        duration: Date.now() - scrapingJob.startedAt.getTime()
      };
      
    } catch (error) {
      logger.error(`‚ùå Scraping failed for source ${sourceId}:`, error);
      
      // Mettre √† jour le job avec l'erreur
      await prisma.scrapingJob.update({
        where: { id: scrapingJob.id },
        data: {
          status: 'FAILED',
          completedAt: new Date(),
          errorMessage: error instanceof Error ? error.message : 'Unknown error'
        }
      });
      
      // Incr√©menter le compteur d'erreurs de la source
      await prisma.source.update({
        where: { id: sourceId },
        data: {
          errorCount: { increment: 1 },
          lastError: error instanceof Error ? error.message : 'Unknown error'
        }
      });
      
      throw error;
    }
  },
  {
    connection: redisConnection,
    concurrency: 3, // 3 scrapers en parall√®le max
    limiter: {
      max: 10, // 10 jobs max
      duration: 60000 // par minute
    }
  }
);

// Gestion des √©v√©nements
scrapingWorker.on('completed', (job) => {
  logger.info(`‚úÖ Job ${job.id} completed`);
});

scrapingWorker.on('failed', (job, err) => {
  logger.error(`‚ùå Job ${job?.id} failed:`, err);
});

scrapingWorker.on('error', (err) => {
  logger.error('Worker error:', err);
});
```

```typescript
// api/src/workers/schedulers/scraping.scheduler.ts
import cron from 'node-cron';
import { prisma } from '@/shared/database/prisma.client';
import { scrapingQueue } from '@/infrastructure/queue/scraping.queue';
import { logger } from '@/infrastructure/logger';

/**
 * Scheduler qui v√©rifie toutes les 5 minutes si des sources
 * doivent √™tre scrapp√©es selon leur scrapingFrequency
 */
export function startScrapingScheduler() {
  logger.info('üïê Starting scraping scheduler (every 5 minutes)');
  
  cron.schedule('*/5 * * * *', async () => {
    try {
      logger.info('‚è∞ Checking sources for scheduled scraping');
      
      // R√©cup√©rer toutes les sources actives
      const sources = await prisma.source.findMany({
        where: { isActive: true }
      });
      
      const now = Date.now();
      let queued = 0;
      
      for (const source of sources) {
        const lastScraped = source.lastScrapedAt?.getTime() || 0;
        const frequency = source.scrapingFrequency * 1000; // Convertir en ms
        const nextScrape = lastScraped + frequency;
        
        // Si c'est le moment de scraper
        if (now >= nextScrape) {
          logger.info(`üì¨ Queueing scraping for source ${source.id} (${source.name})`);
          
          await scrapingQueue.add(
            'scrape-source',
            { sourceId: source.id, force: false },
            {
              jobId: `scrape-${source.id}-${Date.now()}`,
              removeOnComplete: 100, // Garder les 100 derniers jobs
              removeOnFail: 50
            }
          );
          
          queued++;
        }
      }
      
      if (queued > 0) {
        logger.info(`‚úÖ Queued ${queued} scraping jobs`);
      }
      
    } catch (error) {
      logger.error('‚ùå Scheduler error:', error);
    }
  });
}
```

```typescript
// api/src/workers/index.ts
import { scrapingWorker } from './processors/scraping.processor';
import { startScrapingScheduler } from './schedulers/scraping.scheduler';
import { logger } from '@/infrastructure/logger';

async function startWorkers() {
  logger.info('üöÄ Starting workers...');
  
  // D√©marrer le worker de scraping
  logger.info('‚úÖ Scraping worker started');
  
  // D√©marrer le scheduler
  startScrapingScheduler();
  
  // Graceful shutdown
  process.on('SIGTERM', async () => {
    logger.info('SIGTERM received, closing workers...');
    await scrapingWorker.close();
    process.exit(0);
  });
}

startWorkers().catch((error) => {
  logger.error('Failed to start workers:', error);
  process.exit(1);
});
```

**√âtape 1.1.2 : Ajouter script dans package.json**

```json
{
  "scripts": {
    "dev": "tsx watch src/server.ts",
    "workers": "tsx watch src/workers/index.ts",
    "dev:all": "concurrently \"npm run dev\" \"npm run workers\""
  }
}
```

**√âtape 1.1.3 : Tester**

```bash
# Terminal 1 : API
npm run dev

# Terminal 2 : Workers
npm run workers

# Terminal 3 : Cr√©er une source et v√©rifier le scraping
curl -X POST http://localhost:5001/api/v1/sources \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "brandId": "...",
    "type": "TRUSTPILOT",
    "name": "Test Source",
    "config": { "companyName": "example.com" }
  }'

# V√©rifier les logs du worker
# Devrait voir : "üöÄ Starting scraping job for source ..."
```

**Crit√®res de validation:**
- ‚úÖ Worker d√©marre sans erreur
- ‚úÖ Scheduler tourne toutes les 5 minutes
- ‚úÖ Cr√©ation de source d√©clenche un scraping imm√©diat
- ‚úÖ Scraping r√©current fonctionne selon scrapingFrequency

---

### T√¢che 1.2 : Ajouter Pagination Partout ‚è±Ô∏è 1 jour

**Priorit√©:** üî¥ P0 - CRITIQUE  
**Impact:** Risque de timeout et crash avec gros volumes  
**Effort:** 1 jour

#### Probl√®me Actuel
```typescript
// ‚ùå Aucune pagination
async getMentions(filters) {
  return await prisma.mention.findMany({ where: filters });
  // Peut retourner 100,000+ mentions ‚Üí Timeout
}
```

#### Solution

**√âtape 1.2.1 : Cr√©er utilitaire de pagination**

```typescript
// api/src/shared/utils/pagination.ts
import { Prisma } from '@prisma/client';

export interface PaginationParams {
  page?: number;
  limit?: number;
  sortBy?: string;
  sortOrder?: 'asc' | 'desc';
}

export interface PaginatedResponse<T> {
  data: T[];
  pagination: {
    page: number;
    limit: number;
    total: number;
    totalPages: number;
    hasNext: boolean;
    hasPrev: boolean;
  };
}

export async function paginate<T>(
  model: any,
  where: any,
  params: PaginationParams,
  include?: any
): Promise<PaginatedResponse<T>> {
  // Valider et normaliser les param√®tres
  const page = Math.max(1, params.page || 1);
  const limit = Math.min(100, Math.max(1, params.limit || 20));
  const skip = (page - 1) * limit;
  
  // Construire l'orderBy
  const orderBy: any = {};
  if (params.sortBy) {
    orderBy[params.sortBy] = params.sortOrder || 'desc';
  } else {
    orderBy.createdAt = 'desc'; // Par d√©faut
  }
  
  // Ex√©cuter les requ√™tes en parall√®le
  const [data, total] = await Promise.all([
    model.findMany({
      where,
      skip,
      take: limit,
      orderBy,
      include
    }),
    model.count({ where })
  ]);
  
  const totalPages = Math.ceil(total / limit);
  
  return {
    data,
    pagination: {
      page,
      limit,
      total,
      totalPages,
      hasNext: page < totalPages,
      hasPrev: page > 1
    }
  };
}

// Helper pour extraire les params de pagination depuis req.query
export function extractPaginationParams(query: any): PaginationParams {
  return {
    page: query.page ? parseInt(query.page) : 1,
    limit: query.limit ? parseInt(query.limit) : 20,
    sortBy: query.sortBy as string,
    sortOrder: query.sortOrder === 'asc' ? 'asc' : 'desc'
  };
}
```

**√âtape 1.2.2 : Appliquer √† tous les modules**

```typescript
// api/src/modules/mentions/mentions.service.ts
import { paginate, PaginationParams, PaginatedResponse } from '@/shared/utils/pagination';

class MentionsService {
  async getMentions(
    filters: any,
    pagination: PaginationParams
  ): Promise<PaginatedResponse<Mention>> {
    return await paginate(
      prisma.mention,
      filters,
      pagination,
      { brand: true, source: true } // include
    );
  }
}

// api/src/modules/mentions/mentions.controller.ts
import { extractPaginationParams } from '@/shared/utils/pagination';

class MentionsController {
  async getMentions(req: Request, res: Response) {
    const user = req.user;
    const { brandId, sentiment } = req.query;
    
    // Extraire pagination
    const pagination = extractPaginationParams(req.query);
    
    // Construire filtres
    const filters: any = {
      brand: { organizationId: user.organizationId }
    };
    
    if (brandId) filters.brandId = brandId;
    if (sentiment) filters.sentiment = sentiment;
    
    // Appeler service avec pagination
    const result = await mentionsService.getMentions(filters, pagination);
    
    res.json({
      success: true,
      ...result
    });
  }
}
```

**Modules √† mettre √† jour:**
- ‚úÖ mentions (GET /mentions)
- ‚úÖ sources (GET /sources)
- ‚úÖ brands (GET /brands)
- ‚úÖ alerts (GET /alerts)
- ‚úÖ keywords (GET /keywords)
- ‚úÖ actions (GET /actions)
- ‚úÖ reports (GET /reports)
- ‚úÖ users (GET /users)

**√âtape 1.2.3 : Tester**

```bash
# Sans pagination (devrait utiliser d√©fauts)
curl "http://localhost:5001/api/v1/mentions?brandId=xxx"

# Avec pagination
curl "http://localhost:5001/api/v1/mentions?brandId=xxx&page=2&limit=50"

# Avec tri
curl "http://localhost:5001/api/v1/mentions?sortBy=publishedAt&sortOrder=asc"
```

**Crit√®res de validation:**
- ‚úÖ Toutes les routes GET retournent `{ data: [], pagination: {} }`
- ‚úÖ Limite max de 100 items par page
- ‚úÖ Tri fonctionne (asc/desc)
- ‚úÖ Performance : <500ms pour 10,000+ records

---

### T√¢che 1.3 : Corriger Isolation des Donn√©es ‚è±Ô∏è 1 jour

**Priorit√©:** üî¥ P0 - CRITIQUE (S√âCURIT√â)  
**Impact:** Faille de s√©curit√© permettant l'acc√®s aux donn√©es d'autres organisations  
**Effort:** 1 jour

#### Probl√®me Actuel
```typescript
// ‚ùå Module Actions : Pas de filtrage par organization
async getAllActions(req: Request, res: Response) {
  const actions = await prisma.action.findMany();
  // ‚ö†Ô∏è Retourne TOUTES les actions, m√™me celles d'autres orgs
}
```

#### Solution

**√âtape 1.3.1 : Cr√©er middleware de v√©rification d'ownership**

```typescript
// api/src/shared/middleware/ownership.middleware.ts
import { Request, Response, NextFunction } from 'express';
import { prisma } from '@/shared/database/prisma.client';
import { AppError } from '@/shared/utils/errors';

/**
 * V√©rifie que la ressource appartient √† l'organisation de l'utilisateur
 */
export function requireOwnership(resourceType: string, idParam = 'id') {
  return async (req: Request, res: Response, next: NextFunction) => {
    try {
      const resourceId = req.params[idParam];
      const user = req.user;
      
      if (!resourceId) {
        return next();
      }
      
      // Mapper le type de ressource au mod√®le Prisma
      const modelMap: Record<string, any> = {
        'brand': prisma.brand,
        'source': prisma.source,
        'mention': prisma.mention,
        'alert': prisma.alert,
        'action': prisma.action,
        'report': prisma.report,
        'keyword': prisma.keyword
      };
      
      const model = modelMap[resourceType];
      if (!model) {
        throw new AppError(`Unknown resource type: ${resourceType}`, 500);
      }
      
      // Construire la requ√™te selon le type
      let resource;
      
      if (resourceType === 'brand') {
        resource = await model.findFirst({
          where: {
            id: resourceId,
            organizationId: user.organizationId
          }
        });
      } else if (['source', 'mention', 'alert', 'report'].includes(resourceType)) {
        resource = await model.findFirst({
          where: {
            id: resourceId,
            brand: { organizationId: user.organizationId }
          }
        });
      } else if (resourceType === 'action') {
        // Actions peuvent √™tre assign√©es √† un user
        resource = await model.findFirst({
          where: {
            id: resourceId,
            OR: [
              { assignedTo: { organizationId: user.organizationId } },
              { assignedToId: user.userId }
            ]
          }
        });
      }
      
      if (!resource) {
        throw new AppError(
          `${resourceType} not found or access denied`,
          404,
          'RESOURCE_NOT_FOUND'
        );
      }
      
      // Attacher la ressource √† req pour √©viter de la recharger
      req.resource = resource;
      
      next();
    } catch (error) {
      next(error);
    }
  };
}
```

**√âtape 1.3.2 : Appliquer aux routes**

```typescript
// api/src/modules/actions/actions.routes.ts
import { requireOwnership } from '@/shared/middleware/ownership.middleware';

const router = Router();

// Toutes les routes n√©cessitent auth
router.use(requireAuth);

// GET /actions - Filtrer par organization
router.get('/', actionsController.getAllActions);

// Routes avec :id - V√©rifier ownership
router.get('/:id', 
  requireOwnership('action'),
  actionsController.getActionById
);

router.patch('/:id',
  requireOwnership('action'),
  actionsController.updateAction
);

router.delete('/:id',
  requireOwnership('action'),
  actionsController.deleteAction
);
```

```typescript
// api/src/modules/actions/actions.controller.ts
class ActionsController {
  async getAllActions(req: Request, res: Response) {
    const user = req.user;
    
    // ‚úÖ CORRECT : Filtrer par organization
    const actions = await prisma.action.findMany({
      where: {
        OR: [
          { assignedTo: { organizationId: user.organizationId } },
          { assignedToId: user.userId }
        ]
      },
      include: { assignedTo: true }
    });
    
    res.json({ success: true, data: actions });
  }
  
  async getActionById(req: Request, res: Response) {
    // ‚úÖ Ressource d√©j√† v√©rifi√©e par middleware
    const action = req.resource;
    res.json({ success: true, data: action });
  }
}
```

**Modules √† corriger:**
- üî¥ actions (CRITIQUE)
- üü† mentions (v√©rifier)
- üü† sources (v√©rifier)
- üü† alerts (v√©rifier)

**√âtape 1.3.3 : Tests de s√©curit√©**

```typescript
// api/src/__tests__/security/isolation.test.ts
describe('Data Isolation', () => {
  it('should not allow access to other org actions', async () => {
    // Cr√©er 2 organisations
    const org1 = await createOrg('Org 1');
    const org2 = await createOrg('Org 2');
    
    // Cr√©er une action pour org2
    const action = await prisma.action.create({
      data: {
        title: 'Secret Action',
        assignedTo: { connect: { id: org2.ownerId } }
      }
    });
    
    // User de org1 essaie d'acc√©der
    const response = await request(app)
      .get(`/api/v1/actions/${action.id}`)
      .set('Authorization', `Bearer ${org1Token}`);
    
    expect(response.status).toBe(404);
    expect(response.body.error.code).toBe('RESOURCE_NOT_FOUND');
  });
});
```

**Crit√®res de validation:**
- ‚úÖ Impossible d'acc√©der aux ressources d'une autre org
- ‚úÖ GET /actions ne retourne que les actions de l'org
- ‚úÖ GET /actions/:id retourne 404 si autre org
- ‚úÖ Tests de s√©curit√© passent

---

### T√¢che 1.4 : Batch Processing Scrapers ‚è±Ô∏è 1 jour

**Priorit√©:** üî¥ P0 - CRITIQUE (PERFORMANCE)  
**Impact:** Performance x10 sur l'insertion de mentions  
**Effort:** 1 jour

#### Probl√®me Actuel
```python
# ‚ùå 1 query + 1 commit par item
for item in items:
    self.cur.execute(query, ...)
    self.conn.commit()
    
# 10,000 items = 10,000 queries = 5-10 minutes
```

#### Solution

```python
# scrapers/sentinelle_scrapers/pipelines.py
import json
import logging
import os
from datetime import datetime
import psycopg2
from psycopg2.extras import execute_values
from scrapy.exceptions import DropItem

class DatabasePipeline:
    """
    PostgreSQL Pipeline avec batch processing
    """
    def __init__(self, db_config):
        self.db_config = db_config
        self.logger = logging.getLogger(__name__)
        self.items_buffer = []
        self.buffer_size = 100  # Flush tous les 100 items
    
    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            db_config={
                'host': os.getenv('DB_HOST', 'localhost'),
                'database': os.getenv('DB_NAME', 'sentinelle'),
                'user': os.getenv('DB_USER', 'postgres'),
                'password': os.getenv('DB_PASSWORD', ''),
                'port': os.getenv('DB_PORT', '5432'),
            }
        )
    
    def open_spider(self, spider):
        try:
            self.conn = psycopg2.connect(**self.db_config)
            self.cur = self.conn.cursor()
            self.logger.info("‚úÖ Connected to PostgreSQL")
        except Exception as e:
            self.logger.error(f"‚ùå Database connection failed: {e}")
            self.conn = None
    
    def close_spider(self, spider):
        # Flush remaining items
        if self.items_buffer:
            self.flush_buffer()
        
        if hasattr(self, 'conn') and self.conn:
            self.cur.close()
            self.conn.close()
            self.logger.info("‚úÖ Database connection closed")
    
    def process_item(self, item, spider):
        if not hasattr(self, 'conn') or not self.conn:
            return item
        
        source_id = item.get('source_id')
        if not source_id:
            self.logger.error("‚ùå Missing source_id in item. Skipping.")
            raise DropItem("Missing source_id")
        
        # Ajouter au buffer
        self.items_buffer.append({
            'source_id': source_id,
            'platform': item.get('platform'),
            'author': item.get('author', 'Anonyme'),
            'content': item.get('content'),
            'url': item.get('url', ''),
            'published_at': item.get('published_at'),
            'scraped_at': item.get('scraped_at', datetime.now().isoformat()),
            'external_id': item.get('external_id'),
            'rating': item.get('rating'),
            'metadata': json.dumps(item.get('metadata', {}))
        })
        
        # Flush si buffer plein
        if len(self.items_buffer) >= self.buffer_size:
            self.flush_buffer()
        
        return item
    
    def flush_buffer(self):
        """
        Ins√®re tous les items du buffer en une seule requ√™te
        """
        if not self.items_buffer:
            return
        
        try:
            # 1. R√©cup√©rer tous les brandIds en une requ√™te
            source_ids = list(set(item['source_id'] for item in self.items_buffer))
            
            self.cur.execute(
                'SELECT id, "brandId" FROM sources WHERE id = ANY(%s)',
                (source_ids,)
            )
            
            source_to_brand = {row[0]: row[1] for row in self.cur.fetchall()}
            
            # 2. Pr√©parer les valeurs pour batch insert
            values = []
            for item in self.items_buffer:
                brand_id = source_to_brand.get(item['source_id'])
                if not brand_id:
                    self.logger.warning(f"‚ö†Ô∏è Source {item['source_id']} not found, skipping")
                    continue
                
                values.append((
                    brand_id,
                    item['source_id'],
                    item['platform'],
                    item['author'],
                    item['content'],
                    item['url'],
                    item['published_at'],
                    item['scraped_at'],
                    item['external_id'],
                    item['rating'],
                    item['metadata']
                ))
            
            if not values:
                self.logger.warning("‚ö†Ô∏è No valid items to insert")
                self.items_buffer = []
                return
            
            # 3. Batch insert avec ON CONFLICT
            query = """
                INSERT INTO mentions (
                    "brandId", "sourceId", platform, author, content,
                    url, "publishedAt", "scrapedAt", sentiment, "externalId",
                    "rawData"
                ) VALUES %s
                ON CONFLICT ("externalId", platform) DO UPDATE SET
                    content = EXCLUDED.content,
                    "publishedAt" = EXCLUDED."publishedAt",
                    "scrapedAt" = EXCLUDED."scrapedAt"
            """
            
            # Utiliser execute_values pour batch insert
            execute_values(
                self.cur,
                query,
                values,
                template="(%s, %s, %s, %s, %s, %s, %s, %s, 'NEUTRAL', %s, %s)",
                page_size=100
            )
            
            self.conn.commit()
            
            self.logger.info(f"üíæ Batch inserted {len(values)} mentions")
            
        except Exception as e:
            self.logger.error(f"‚ùå Batch insert failed: {e}")
            self.conn.rollback()
        finally:
            # Vider le buffer
            self.items_buffer = []
```

**√âtape 1.4.2 : Tester la performance**

```python
# scrapers/test_batch_performance.py
import time
import psycopg2
from psycopg2.extras import execute_values

def test_single_insert(conn, items):
    """M√©thode actuelle (lente)"""
    cur = conn.cursor()
    start = time.time()
    
    for item in items:
        cur.execute("INSERT INTO mentions (...) VALUES (%s, %s, ...)", item)
        conn.commit()
    
    duration = time.time() - start
    print(f"Single insert: {duration:.2f}s for {len(items)} items")
    return duration

def test_batch_insert(conn, items):
    """M√©thode optimis√©e (rapide)"""
    cur = conn.cursor()
    start = time.time()
    
    execute_values(cur, "INSERT INTO mentions (...) VALUES %s", items)
    conn.commit()
    
    duration = time.time() - start
    print(f"Batch insert: {duration:.2f}s for {len(items)} items")
    return duration

# R√©sultats attendus:
# Single insert: 45.23s for 1000 items
# Batch insert: 1.12s for 1000 items
# Am√©lioration: 40x plus rapide
```

**Crit√®res de validation:**
- ‚úÖ Batch insert fonctionne
- ‚úÖ Performance : <5s pour 10,000 items (vs 5min avant)
- ‚úÖ ON CONFLICT fonctionne (pas de doublons)
- ‚úÖ Pas de perte de donn√©es

---

### T√¢che 1.5 : Validation Zod Syst√©matique ‚è±Ô∏è 1 jour

**Priorit√©:** üü† P1 - IMPORTANT  
**Impact:** Pr√©vention des erreurs runtime et donn√©es invalides  
**Effort:** 1 jour

#### Probl√®me Actuel
```typescript
// ‚ùå Validation manuelle incoh√©rente
if (!brandId || !type || !name) {
  res.status(400).json({ error: 'Missing fields' });
}

// ‚ùå Pas de validation de format
const email = req.body.email; // Peut √™tre n'importe quoi
```

#### Solution

**√âtape 1.5.1 : Cr√©er sch√©mas Zod centralis√©s**

```typescript
// shared/validators/schemas.ts
import { z } from 'zod';

// ========== AUTH ==========
export const registerSchema = z.object({
  email: z.string().email('Email invalide'),
  password: z.string().min(8, 'Mot de passe trop court (min 8 caract√®res)'),
  name: z.string().min(2, 'Nom trop court').optional(),
  organizationName: z.string().min(2, 'Nom d\'organisation requis')
});

export const loginSchema = z.object({
  email: z.string().email('Email invalide'),
  password: z.string().min(1, 'Mot de passe requis')
});

// ========== SOURCES ==========
export const createSourceSchema = z.object({
  brandId: z.string().cuid('Brand ID invalide'),
  type: z.enum([
    'TRUSTPILOT', 'GOOGLE_REVIEWS', 'NEWS', 'BLOG', 
    'FORUM', 'RSS', 'REVIEW', 'OTHER'
  ]),
  name: z.string().min(3, 'Nom trop court').max(100, 'Nom trop long'),
  config: z.object({
    companyName: z.string().optional(),
    url: z.string().url('URL invalide').optional(),
    apiKey: z.string().optional(),
    keywords: z.array(z.string()).optional()
  }),
  scrapingFrequency: z.number().int().min(1800).max(86400).optional()
    .describe('Fr√©quence en secondes (30min - 24h)')
});

export const updateSourceSchema = createSourceSchema.partial();

// ========== MENTIONS ==========
export const createMentionSchema = z.object({
  brandId: z.string().cuid(),
  sourceId: z.string().cuid(),
  content: z.string().min(1).max(10000),
  author: z.string().max(255),
  url: z.string().url(),
  publishedAt: z.string().datetime().or(z.date()),
  platform: z.string(),
  externalId: z.string(),
  sentiment: z.enum(['POSITIVE', 'NEGATIVE', 'NEUTRAL', 'MIXED']).optional()
});

export const searchMentionsSchema = z.object({
  brandId: z.string().cuid().optional(),
  sentiment: z.enum(['POSITIVE', 'NEGATIVE', 'NEUTRAL', 'MIXED']).optional(),
  startDate: z.string().datetime().optional(),
  endDate: z.string().datetime().optional(),
  keywords: z.array(z.string()).optional(),
  page: z.number().int().min(1).optional(),
  limit: z.number().int().min(1).max(100).optional()
});

// ========== BRANDS ==========
export const createBrandSchema = z.object({
  name: z.string().min(2).max(100),
  description: z.string().max(500).optional(),
  website: z.string().url().optional(),
  keywords: z.array(z.string()).max(50)
});

// ========== ALERTS ==========
export const createAlertSchema = z.object({
  brandId: z.string().cuid(),
  name: z.string().min(3).max(100),
  description: z.string().max(500).optional(),
  condition: z.enum([
    'NEGATIVE_SENTIMENT_THRESHOLD',
    'KEYWORD_FREQUENCY',
    'MENTION_SPIKE',
    'SENTIMENT_DROP',
    'CUSTOM'
  ]),
  threshold: z.number().min(0).max(100),
  level: z.enum(['LOW', 'MEDIUM', 'HIGH', 'CRITICAL']).optional()
});

// ========== ACTIONS ==========
export const createActionSchema = z.object({
  title: z.string().min(3).max(200),
  description: z.string().max(2000).optional(),
  priority: z.number().int().min(0).max(10).optional(),
  dueDate: z.string().datetime().optional(),
  assignedToId: z.string().cuid().optional(),
  tags: z.array(z.string()).max(10).optional()
});
```

**√âtape 1.5.2 : Appliquer aux routes**

```typescript
// Tous les modules
import { validate } from '@/shared/middleware/validate.middleware';
import { createSourceSchema, updateSourceSchema } from '@/shared/validators/schemas';

router.post('/',
  requireAuth,
  validate(createSourceSchema),
  sourcesController.createSource
);

router.patch('/:id',
  requireAuth,
  validate(updateSourceSchema),
  sourcesController.updateSource
);
```

**Modules √† mettre √† jour:**
- ‚úÖ sources
- ‚úÖ mentions
- ‚úÖ brands
- ‚úÖ alerts
- ‚úÖ actions
- ‚úÖ keywords
- ‚úÖ reports

**Crit√®res de validation:**
- ‚úÖ Toutes les routes POST/PATCH ont validate()
- ‚úÖ Erreurs 422 avec d√©tails clairs
- ‚úÖ Pas d'erreurs runtime dues √† donn√©es invalides

---

## üü† PHASE 2 : AM√âLIORATIONS IMPORTANTES (Semaine 2)

### T√¢che 2.1 : Repository Pattern ‚è±Ô∏è 2 jours

**Priorit√©:** üü° P2 - MOYEN  
**Impact:** Meilleure maintenabilit√© et testabilit√©  
**Effort:** 2 jours

#### Objectif
S√©parer la logique d'acc√®s aux donn√©es (DB) de la logique m√©tier

#### Structure Cible

```typescript
// api/src/modules/mentions/mentions.repository.ts
export class MentionsRepository {
  async findById(id: string) {
    return prisma.mention.findUnique({
      where: { id },
      include: { brand: true, source: true }
    });
  }
  
  async findByBrand(brandId: string, filters: any) {
    return prisma.mention.findMany({
      where: { brandId, ...filters },
      include: { brand: true, source: true }
    });
  }
  
  async create(data: CreateMentionDTO) {
    return prisma.mention.create({ data });
  }
  
  async update(id: string, data: Partial<CreateMentionDTO>) {
    return prisma.mention.update({ where: { id }, data });
  }
  
  async delete(id: string) {
    return prisma.mention.delete({ where: { id } });
  }
  
  async countByBrand(brandId: string) {
    return prisma.mention.count({ where: { brandId } });
  }
  
  async findBySentiment(brandId: string, sentiment: string) {
    return prisma.mention.findMany({
      where: { brandId, sentiment }
    });
  }
}

// api/src/modules/mentions/mentions.service.ts
export class MentionsService {
  constructor(private repo: MentionsRepository) {}
  
  async getMentions(brandId: string, filters: any) {
    // Logique m√©tier
    const mentions = await this.repo.findByBrand(brandId, filters);
    
    // Enrichissement, transformations, etc.
    return mentions.map(m => this.enrichMention(m));
  }
  
  private enrichMention(mention: Mention) {
    // Logique m√©tier
    return {
      ...mention,
      sentimentLabel: this.getSentimentLabel(mention.sentiment)
    };
  }
}
```

**Modules √† refactorer:**
- mentions
- sources
- brands
- alerts

---

### T√¢che 2.2 : Rate Limiting par Utilisateur ‚è±Ô∏è 1 jour

**Priorit√©:** üü† P1 - IMPORTANT (S√âCURIT√â)  
**Impact:** Pr√©vention des abus  
**Effort:** 1 jour

```typescript
// api/src/shared/middleware/rate-limit.middleware.ts
import rateLimit from 'express-rate-limit';
import RedisStore from 'rate-limit-redis';
import { redisClient } from '@/config/redis';

export const userRateLimiter = rateLimit({
  store: new RedisStore({
    client: redisClient,
    prefix: 'rl:user:'
  }),
  windowMs: 15 * 60 * 1000, // 15 minutes
  max: 100, // 100 requ√™tes par utilisateur
  keyGenerator: (req) => {
    // Utiliser userId si authentifi√©, sinon IP
    return req.user?.userId || req.ip;
  },
  handler: (req, res) => {
    res.status(429).json({
      success: false,
      error: {
        code: 'RATE_LIMIT_EXCEEDED',
        message: 'Trop de requ√™tes. R√©essayez dans 15 minutes.'
      }
    });
  }
});

// Appliquer globalement
app.use('/api/v1', userRateLimiter);
```

---

### T√¢che 2.3 : Monitoring & Alertes ‚è±Ô∏è 2 jours

**Priorit√©:** üü† P1 - IMPORTANT  
**Impact:** D√©tection proactive des probl√®mes  
**Effort:** 2 jours

```typescript
// api/src/infrastructure/monitoring/prometheus.ts
import promClient from 'prom-client';

// M√©triques
export const httpRequestDuration = new promClient.Histogram({
  name: 'http_request_duration_seconds',
  help: 'Duration of HTTP requests in seconds',
  labelNames: ['method', 'route', 'status_code']
});

export const scrapingJobsTotal = new promClient.Counter({
  name: 'scraping_jobs_total',
  help: 'Total number of scraping jobs',
  labelNames: ['status', 'source_type']
});

export const mentionsCreated = new promClient.Counter({
  name: 'mentions_created_total',
  help: 'Total number of mentions created'
});

// Endpoint metrics
app.get('/metrics', async (req, res) => {
  res.set('Content-Type', promClient.register.contentType);
  res.end(await promClient.register.metrics());
});
```

---

### T√¢che 2.4 : Tests Automatis√©s ‚è±Ô∏è 3 jours

**Priorit√©:** üü° P2 - MOYEN  
**Impact:** Pr√©vention des r√©gressions  
**Effort:** 3 jours

```typescript
// api/src/__tests__/integration/sources.test.ts
describe('Sources API', () => {
  it('should create a source', async () => {
    const response = await request(app)
      .post('/api/v1/sources')
      .set('Authorization', `Bearer ${token}`)
      .send({
        brandId: testBrand.id,
        type: 'TRUSTPILOT',
        name: 'Test Source',
        config: { companyName: 'example.com' }
      });
    
    expect(response.status).toBe(201);
    expect(response.body.data.name).toBe('Test Source');
  });
  
  it('should trigger scraping on creation', async () => {
    // V√©rifier que le job est dans la queue
    const jobs = await scrapingQueue.getJobs(['waiting']);
    expect(jobs.length).toBeGreaterThan(0);
  });
});
```

**Couverture cible:**
- ‚úÖ Auth (login, register, JWT)
- ‚úÖ Sources (CRUD + scraping)
- ‚úÖ Mentions (CRUD + pagination)
- ‚úÖ S√©curit√© (isolation, ownership)

---

## üü° PHASE 3 : OPTIMISATIONS (Semaine 3)

### T√¢che 3.1 : Caching Redis ‚è±Ô∏è 2 jours

```typescript
// api/src/infrastructure/cache/redis.service.ts
export class CacheService {
  async get<T>(key: string): Promise<T | null> {
    const value = await redisClient.get(key);
    return value ? JSON.parse(value) : null;
  }
  
  async set(key: string, value: any, ttl = 3600) {
    await redisClient.setex(key, ttl, JSON.stringify(value));
  }
  
  async invalidate(pattern: string) {
    const keys = await redisClient.keys(pattern);
    if (keys.length) await redisClient.del(...keys);
  }
}

// Utilisation
async getAnalyticsSummary(brandId: string) {
  const cacheKey = `analytics:summary:${brandId}`;
  
  // V√©rifier cache
  const cached = await cacheService.get(cacheKey);
  if (cached) return cached;
  
  // Calculer
  const summary = await this.calculateSummary(brandId);
  
  // Mettre en cache (15 minutes)
  await cacheService.set(cacheKey, summary, 900);
  
  return summary;
}
```

---

### T√¢che 3.2 : Database Indexes ‚è±Ô∏è 1 jour

```sql
-- Indexes pour performance
CREATE INDEX CONCURRENTLY idx_mentions_brand_sentiment_date
ON mentions(brandId, sentiment, publishedAt DESC);

CREATE INDEX CONCURRENTLY idx_mentions_search
ON mentions USING gin(to_tsvector('french', content));

CREATE INDEX CONCURRENTLY idx_sources_active_frequency
ON sources(isActive, scrapingFrequency) WHERE isActive = true;

-- Analyser les requ√™tes lentes
SELECT query, calls, total_time, mean_time
FROM pg_stat_statements
ORDER BY mean_time DESC
LIMIT 10;
```

---

### T√¢che 3.3 : Archivage Automatique ‚è±Ô∏è 2 jours

```typescript
// api/src/workers/jobs/archiving.job.ts
cron.schedule('0 2 * * *', async () => {
  const sixMonthsAgo = new Date();
  sixMonthsAgo.setMonth(sixMonthsAgo.getMonth() - 6);
  
  // Archiver vers S3
  const oldMentions = await prisma.mention.findMany({
    where: { createdAt: { lt: sixMonthsAgo } },
    take: 10000
  });
  
  if (oldMentions.length > 0) {
    await s3.upload({
      Bucket: 'sentinelle-archive',
      Key: `mentions-${Date.now()}.json.gz`,
      Body: gzip(JSON.stringify(oldMentions))
    });
    
    // Supprimer de la DB
    await prisma.mention.deleteMany({
      where: {
        id: { in: oldMentions.map(m => m.id) }
      }
    });
    
    logger.info(`Archived ${oldMentions.length} old mentions`);
  }
});
```

---

## üìä TABLEAU DE BORD DU PROJET

### Checklist Globale

#### üî¥ Phase 1 : Corrections Critiques (Semaine 1)
- [ ] 1.1 Workers BullMQ (2j)
- [ ] 1.2 Pagination (1j)
- [ ] 1.3 Isolation donn√©es (1j)
- [ ] 1.4 Batch processing (1j)
- [ ] 1.5 Validation Zod (1j)

#### üü† Phase 2 : Am√©liorations (Semaine 2)
- [ ] 2.1 Repository pattern (2j)
- [ ] 2.2 Rate limiting user (1j)
- [ ] 2.3 Monitoring (2j)
- [ ] 2.4 Tests automatis√©s (3j)

#### üü° Phase 3 : Optimisations (Semaine 3)
- [ ] 3.1 Caching Redis (2j)
- [ ] 3.2 Database indexes (1j)
- [ ] 3.3 Archivage auto (2j)

---

## üéØ CRIT√àRES DE SUCC√àS

### Avant Production

**S√©curit√©:**
- ‚úÖ Toutes les routes prot√©g√©es par requireAuth
- ‚úÖ Isolation des donn√©es v√©rifi√©e (tests passent)
- ‚úÖ Rate limiting par utilisateur actif
- ‚úÖ Validation Zod sur tous les inputs

**Performance:**
- ‚úÖ Pagination sur toutes les routes GET
- ‚úÖ Temps de r√©ponse <500ms (95e percentile)
- ‚úÖ Batch processing scrapers (>10x plus rapide)
- ‚úÖ Indexes DB optimis√©s

**Fiabilit√©:**
- ‚úÖ Workers automatiques fonctionnels
- ‚úÖ Scraping r√©current op√©rationnel
- ‚úÖ Gestion d'erreurs robuste (retry, logs)
- ‚úÖ Tests de couverture >70%

**Monitoring:**
- ‚úÖ M√©triques Prometheus expos√©es
- ‚úÖ Logs structur√©s (JSON)
- ‚úÖ Alertes configur√©es (downtime, erreurs)

---

## üìû SUPPORT & RESSOURCES

### Documentation
- Architecture compl√®te : `ARCHITECTURE_ANALYSIS.md`
- Plan d'action : `ACTION_PLAN.md` (ce fichier)
- README : `README.md`

### Commandes Utiles

```bash
# D√©marrer tout en dev
npm run dev:all

# Workers seuls
npm run workers

# Tests
npm test

# V√©rifier la queue Redis
redis-cli
> KEYS scraping:*
> LLEN scraping:waiting

# V√©rifier les jobs en DB
psql -d sentinelle -c "SELECT * FROM scraping_jobs ORDER BY createdAt DESC LIMIT 10;"
```

### Contacts
- **D√©veloppeur:** [Votre nom]
- **Repo:** https://github.com/...
- **Docs API:** http://localhost:5001/api/v1/docs

---

**Derni√®re mise √† jour:** 7 F√©vrier 2026
