# ğŸ”¬ AUDIT TECHNIQUE COMPLET â€” SYSTÃˆME DE SCRAPING

## ğŸ“‹ RÃ‰SUMÃ‰ EXÃ‰CUTIF

Ce rapport constitue un audit technique approfondi du **systÃ¨me de scraping** de Sentinelle Reputation. L'analyse couvre l'architecture globale, le pipeline de traitement, la qualitÃ© du scraping, la gestion des donnÃ©es, la performance, la sÃ©curitÃ©, l'observabilitÃ© et l'intÃ©gration avec le reste du systÃ¨me.

**Score global estimÃ©**: 6.8/10  
**Statut**: Fonctionnel mais avec des amÃ©liorations critiques nÃ©cessaires

---

## 1ï¸âƒ£ ARCHITECTURE GLOBALE

### 1.1 Diagramme Logique du SystÃ¨me

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        SYSTÃˆME DE SCRAPING SENTINELLE                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   USER UI    â”‚     â”‚  ADMIN API   â”‚     â”‚   SCHEDULER  â”‚
â”‚  (Frontend)  â”‚     â”‚ (REST API)   â”‚     â”‚   (Cron)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                    â”‚                    â”‚
       â”‚ POST /sources      â”‚                    â”‚ Trigger
       â”‚                    â”‚                    â”‚
       â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      API GATEWAY (Express)                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Auth Middle â”‚  â”‚ Rate Limiter â”‚  â”‚ Ownership Middleware  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MODULES SOURCES                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ SourcesService   â”‚  â”‚SourceAnalyzer    â”‚  â”‚SourcesRoutes  â”‚  â”‚
â”‚  â”‚ (CRUD + Config)  â”‚  â”‚ (URL Diagnostic)â”‚  â”‚   (REST)      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                     â”‚                     â”‚
            â–¼                     â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BULLET QUEUE (BullMQ)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  scrapingQueue                                               â”‚  â”‚
â”‚  â”‚  - Retry: 3 (exponential backoff)                          â”‚  â”‚
â”‚  â”‚  - Concurrency: 5                                         â”‚  â”‚
â”‚  â”‚  - Limiter: 10 req/sec                                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   SCRAPING WORKER                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  processScrapingJob()                                       â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚  â”‚
â”‚  â”‚  â”‚callScraperApi() â”‚  â”‚runScrapyLocal()  â”‚               â”‚  â”‚
â”‚  â”‚  â”‚   (Python API)  â”‚  â”‚   (Fallback)     â”‚               â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                                           â”‚
         â–¼                                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PYTHON SCRAPER API  â”‚              â”‚    LOCAL SCRAPY          â”‚
â”‚  (ai-service)        â”‚              â”‚    (Fallback spider)     â”‚
â”‚  - Google Reviews    â”‚              â”‚  - Trustpilot            â”‚
â”‚  - Trustpilot        â”‚              â”‚  - Google Reviews        â”‚
â”‚  - Twitter           â”‚              â”‚  - News                  â”‚
â”‚  - Reddit            â”‚              â”‚                          â”‚
â”‚  - YouTube           â”‚              â”‚                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ HTTP Response (JSON)
         â”‚ { data: [ScrapedItem] }
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              MENTIONS SERVICE                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  createFromScraper()                                       â”‚  â”‚
â”‚  â”‚  - Deduplication (externalId + platform)                  â”‚  â”‚
â”‚  â”‚  - Cache invalidation                                     â”‚  â”‚
â”‚  â”‚  - Analytics update                                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    POSTGRES DATABASE                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Source  â”‚  â”‚ Mention  â”‚  â”‚ ScrapingJob  â”‚  â”‚BrandMetricsâ”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                      â”‚
â”‚  Index:                                                             â”‚
â”‚  - mentions: (externalId, platform) UNIQUE                         â”‚
â”‚  - mentions: (brandId, sentiment, publishedAt) COMPOSITE            â”‚
â”‚  - sources: (isActive, scrapingFrequency)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â”‚ Data available
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FRONTEND DISPLAY                                 â”‚
â”‚  - Dashboard: Real-time mention count                            â”‚
â”‚  - Mentions: Paginated list with filters                         â”‚
â”‚  - Analytics: Sentiment trends                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 Composants ImpliquÃ©s

| Composant | Type | Fichier Principal | Responsibility |
|-----------|------|------------------|----------------|
| **SourcesService** | Service | [`sources.service.ts`](api/src/modules/sources/sources.service.ts) | CRUD sources, validation config |
| **SourceAnalyzer** | Service | [`source-analyzer.ts`](api/src/modules/sources/source-analyzer.ts) | Diagnostic URLs |
| **SourceAnalyzerService** | Service | [`source-analyzer.service.ts`](api/src/modules/sources/source-analyzer.service.ts) | Integration analyzer |
| **ScrapingWorker** | Worker | [`scraping.worker.ts`](api/src/infrastructure/worker/scraping.worker.ts) | Job processing |
| **ScrapingProcessor** | Processor | [`scraping.processor.ts`](api/src/workers/processors/scraping.processor.ts) | Core scraping logic |
| **ScrapingQueue** | Queue | [`scraping.queue.ts`](api/src/infrastructure/queue/scraping.queue.ts) | Job queue management |
| **MentionsService** | Service | [`mentions.service.ts`](api/src/modules/mentions/mentions.service.ts) | Mention storage |

### 1.3 Flux de DonnÃ©es Complet

```
1. DÃ‰CLENCHEMENT
   â”œâ”€â”€ Manuel: POST /api/v1/sources/:sourceId/scrape
   â”œâ”€â”€ ProgrammÃ©: Scheduler â†’ BullMQ.add()
   â””â”€â”€ API: SourceAnalyzer â†’ createSourceFromDiagnostic()

2. COLLECTE
   â”œâ”€â”€ Source externe (Google Reviews, Trustpilot, etc.)
   â”œâ”€â”€ Python Scraper API (ai-service)
   â””â”€â”€ Fallback: Scrapy local

3. TRAITEMENT
   â”œâ”€â”€ Validation des donnÃ©es
   â”œâ”€â”€ Normalisation (ScrapedItem)
   â””â”€â”€ DÃ©tection des blocages

4. STOCKAGE
   â”œâ”€â”€ DÃ©duplication (externalId + platform)
   â”œâ”€â”€ CrÃ©ation Mention
   â”œâ”€â”€ Invalidation cache analytics
   â””â”€â”€ Mise Ã  jour Source (lastScrapedAt, errorCount)

5. DISTRIBUTION
   â”œâ”€â”€ API â†’ Frontend (polling/WebSocket)
   â””â”€â”€ Notifications (si alert trigger)
```

### 1.4 Identification des Couplages

#### Couplages Forts (âš ï¸ Risque)
- **ScrapingWorker â†’ ScrapingProcessor**: DÃ©pendance directe sans interface
- **ScrapingProcessor â†’ SourcesRepository**: Instanciation directe dans le processor
- **SourcesService â†’ scrapingQueue**: Couplage fort avec BullMQ

#### Couplages Faibles (âœ… Bon)
- **SourceAnalyzer â†’ Sources**: Analyse sans dÃ©pendance directe
- **MentionsService â†’ Analytics**: Communication via invalidation cache

---

## 2ï¸âƒ£ PIPELINE DE SCRAPING

### 2.1 DÃ©clenchement

| Type | ImplÃ©mentation | Status |
|------|---------------|--------|
| **Manuel** | [`sources.controller.ts:97`](api/src/modules/sources/sources.controller.ts:97) | âœ… OK |
| **ProgrammÃ©** | BullMQ scheduled jobs | âš ï¸ Partiel |
| **Webhook** | Non implÃ©mentÃ© | âŒ Manquant |
| **Batch** | SourceAnalyzer batch | âœ… OK |

**ProblÃ¨me identifiÃ©**: Le scheduler n'est pas clairement implÃ©mentÃ©. La mÃ©thode [`findPendingSources()`](api/src/modules/sources/sources.repository.ts:75) existe mais aucun cron ne l'appelle.

### 2.2 Pipeline de Traitement

#### Ã‰tape 1: Validation Source
```typescript
// scraping.processor.ts:47-54
const source = await sourcesRepository.findById(sourceId);
if (!source) throw new AppError('Source non trouvÃ©e', 404);
if (!source.isActive && !force) return { success: true, skipped: true };
```

#### Ã‰tape 2: Collecte
```typescript
// scraping.processor.ts:59-70
// Try Python API first
if (scraperUrl) scrapedData = await callScraperApi(type, config, job);
// Fallback to local Scrapy
if (!scrapedData) scrapedData = await runScrapyLocally(sourceId, type, config, source, job);
```

#### Ã‰tape 3: Parsing/Normalisation
- Le parsing est dÃ©lÃ©guÃ© au scraper Python
- Normalisation via `ScrapedItem` interface
- fields: externalId, content, author, publishedAt, url, metadata

#### Ã‰tape 4: Stockage
```typescript
// scraping.processor.ts:223-264
for (const item of scrapedData) {
  await mentionsService.createFromScraper({...});
}
```

### 2.3 Points de DÃ©faillance IdentifiÃ©s

| Point | Risque | GravitÃ© |
|-------|--------|---------|
| **Scraper API unavailable** | Le fallback Scrapy n'est pas fiable | CRITIQUE |
| **Python service down** | Plus de collecte | MAJEUR |
| **Rate limit externe** | Perte de donnÃ©es | MAJEUR |
| **DB connection lost** | Job Ã©choue, retry | MOYEN |
| **Duplicate handling** | Error silently ignored | MINEUR |

### 2.4 Risques de Duplication

**MÃ©canisme actuel**: Contrainte UNIQUE sur `(externalId, platform)` dans [`schema.prisma:213`](database/prisma/schema.prisma:213)

```prisma
@@unique([externalId, platform])
```

**ProblÃ¨me**: En cas d'erreur aprÃ¨s la crÃ©ation partielle, des doublons peuvent apparaÃ®tre si le job est relancÃ©.

### 2.5 Latence et Goulots d'Ã‰tranglement

| Ã‰tape | Latence typique | ProblÃ¨me |
|-------|-----------------|----------|
| API call (Google) | 2-10s | Timeout 300s |
| Parsing HTML | 100-500ms | CPU-bound |
| DB insert (100 mentions) | 200-500ms | N+1 possible |
| Cache invalidation | 50-100ms | RÃ©seau |

---

## 3ï¸âƒ£ QUALITÃ‰ DU SCRAPING

### 3.1 FiabilitÃ© des Sources

| Source | Support | FiabilitÃ© |
|--------|---------|-----------|
| Google Reviews | âœ… API | Haute |
| Trustpilot | âœ… API + Scrapy | Moyenne |
| Twitter/X | âœ… API | Haute |
| Reddit | âœ… API | Haute |
| RSS | âœ… Scrapy | Haute |
| News (Google) | âœ… API | Haute |
| YouTube | âœ… API | Haute |
| Facebook | âš ï¸ LimitÃ© | Faible |
| Instagram | âš ï¸ LimitÃ© | Faible |

### 3.2 Gestion des Erreurs RÃ©seau

**Retry configurÃ©** ([`scraping.queue.ts:19-24`](api/src/infrastructure/queue/scraping.queue.ts:19)):
```typescript
defaultJobOptions: {
  attempts: 3,
  backoff: { type: 'exponential', delay: 2000 }
}
```

**ProblÃ¨mes identifiÃ©s**:
- âŒ Pas de jitter sur le retry (risque de thundering herd)
- âŒ Pas de circuit breaker
- âŒ Retry illimitÃ© si le service externe est durablement down

### 3.3 Timeout

| OpÃ©ration | Timeout | Configuration |
|-----------|---------|---------------|
| HTTP Scraping | 300s | [`scraping.processor.ts:133`](api/src/workers/processors/scraping.processor.ts:133) |
| SourceAnalyzer | 10s | [`source-analyzer.ts:99`](api/src/modules/sources/source-analyzer.ts:99) |
| Head request | 10s | [`source-analyzer.ts:107`](api/src/modules/sources/source-analyzer.ts:107) |

### 3.4 Rotation de Proxy / IP

**Statut**: âŒ **NON IMPLÃ‰MENTÃ‰**

Il n'y a pas de systÃ¨me de rotation de proxy dans le code. Cela pose problÃ¨me pour:
- Google Reviews (rate limit strict)
- Sites anti-bot (Cloudflare, etc.)

### 3.5 Anti-Bot Handling

**ImplÃ©mentÃ©** ([`source-analyzer.ts:386-410`](api/src/modules/sources/source-analyzer.ts:386)):
- âœ… DÃ©tection Cloudflare
- âœ… DÃ©tection reCAPTCHA
- âœ… DÃ©tection WAF
- âš ï¸ Contournement non implÃ©mentÃ©

### 3.6 Gestion des Captchas

**Statut**: âŒ **NON IMPLÃ‰MENTÃ‰**

En cas de dÃ©tection de captcha, le systÃ¨me retourne simplement une erreur. Pas de:
- Service de rÃ©solution
- IntÃ©gration 2Captche, Anti-Captcha
- Notification admin

---

## 4ï¸âƒ£ GESTION DES DONNÃ‰ES

### 4.1 ModÃ¨le de DonnÃ©es

```prisma
model Source {
  id                String        @id
  type              SourceType    // GOOGLE_REVIEWS, TRUSTPILOT, etc.
  name              String
  url               String?
  config            Json?         // API keys, credentials
  isActive          Boolean       @default(true)
  lastScrapedAt     DateTime?
  scrapingFrequency Int           @default(21600)  // secondes
  errorCount        Int           @default(0)
  lastError         String?
  brandId           String
  mentions          Mention[]
}

model Mention {
  id               String       @id
  brandId          String
  sourceId         String
  content          String
  author           String
  url              String
  publishedAt      DateTime
  platform         SourceType
  externalId       String       // ID externe unique
  sentiment        SentimentType @default(NEUTRAL)
  // ... other fields
  
  @@unique([externalId, platform])
}

model ScrapingJob {
  id              String    @id
  sourceId        String
  status          String
  startedAt       DateTime?
  completedAt     DateTime?
  mentionsFound   Int       @default(0)
  mentionsCreated Int       @default(0)
  errorMessage    String?
  
  @@index([sourceId, status])
}
```

### 4.2 Normalisation

**Points positifs**:
- âœ… Unified `Mention` model pour toutes les sources
- âœ… Normalisation des dates (`publishedAt`)
- âœ… Sentiment analysis intÃ©grÃ©

**ProblÃ¨mes**:
- âŒ Champs spÃ©cifiques perdus (ex: rating sur Google Reviews)
- âŒ Metadata stockÃ©e en JSON brut non structurÃ©

### 4.3 Indexation

| Table | Index | Status |
|-------|-------|--------|
| mentions | (externalId, platform) UNIQUE | âœ… OK |
| mentions | (brandId, sentiment, publishedAt) | âœ… OK |
| sources | (brandId) | âœ… OK |
| sources | (isActive, scrapingFrequency) | âœ… OK |
| scraping_jobs | (sourceId, status) | âœ… OK |

**Manquant**:
- âŒ Index sur `mentions.publishedAt` pour les queries temporelles
- âŒ Index sur `sources.lastScrapedAt` pour le scheduler

### 4.4 DÃ©duplication

**MÃ©canisme**: Contrainte UNIQUE + gestion d'erreur

```typescript
// scraping.processor.ts:254-258
if (error.code === 'P2002') {
  logger.debug(`Duplicate mention skipped: ${item.externalId}`);
  continue;
}
```

**ProblÃ¨me**: Les doublons sont simplement ignorÃ©s sans traÃ§abilitÃ©.

### 4.5 Gestion des Conflits

**Statut**: âš ï¸ **PARTIEL**

- Les conflits d'unicitÃ© sont gÃ©rÃ©s
- Pas de stratÃ©gie pour les mises Ã  jour de mentions existantes
- Pas de versioning des mentions

---

## 5ï¸âƒ£ PERFORMANCE & SCALABILITÃ‰

### 5.1 Concurrence des Workers

**Configuration actuelle** ([`scraping.worker.ts:33`](api/src/infrastructure/worker/scraping.worker.ts:33)):
```typescript
concurrency: parseInt(process.env.SCRAPING_CONCURRENCY || '5'),
limiter: { max: 10, duration: 1000 }
```

**Analyse**:
- âœ… 5 workers parallÃ¨les
- âœ… Rate limit: 10 req/sec
- âš ï¸ Pas de scaling automatique

### 5.2 ParallÃ©lisation

| Niveau | ImplÃ©mentation | Status |
|--------|----------------|--------|
| Batch URL analysis | Promise.all | âœ… OK |
| Multiple sources | BullMQ concurrency | âœ… OK |
| Mention creation | Sequential loop | âŒ Non parallÃ©lisÃ© |

### 5.3 Utilisation Ressources

| Ressource | Estimation | Note |
|-----------|------------|------|
| CPU | 1-2 cores | I/O bound |
| RAM | 512MB-1GB | DÃ©pend du HTML |
| RÃ©seau | Ã‰levÃ© | API calls externes |
| DB | ModÃ©rÃ© | Insert mentions |

### 5.4 Limites Actuelles

| MÃ©trique | Limite | Commentaire |
|----------|--------|-------------|
| Sources actives/org | Plan-based | 3-100 |
| Workers | 5 (fixe) | Non Ã©lastique |
| Rate limit external | Variable | DÃ©pend de la source |
| Timeout scraping | 300s | Configurable |

### 5.5 CapacitÃ© de MontÃ©e en Charge

**Estimations**:
- **100 organisations**: âœ… GÃ©rable (5 workers suffisent)
- **1000 organisations**: âš ï¸ Attention aux limites API externes
- **10,000 organisations**: âŒ NÃ©cessite refonte (workers Ã©lastique, queue partitionnÃ©e)

---

## 6ï¸âƒ£ SÃ‰CURITÃ‰

### 6.1 Risques LÃ©gaux du Scraping

| Risque | Status | Mitigation |
|--------|--------|------------|
| Violation ToS | âš ï¸ Present | robots.txt check |
| Copyright | âš ï¸ Present | Limiter le stockage |
| LGPD/GDPR | âš ï¸ DonnÃ©es personnelles | Anonymisation? |

### 6.2 Protection contre Injection

**Status**: âœ… **PARTIEL**

- âœ… SQL Injection: Prisma parameterized queries
- âœ… XSS: DonnÃ©es stockÃ©es, pas affichÃ©es directement
- âš ï¸ Config injection: Les credentials sont en JSON dans la DB

### 6.3 Stockage des Credentials

**ProblÃ¨me critique** ([`sources.service.ts:212-293`](api/src/modules/sources/sources.service.ts:212)):

Les clÃ©s API (Google API, Twitter Bearer, etc.) sont stockÃ©es en plaintext dans `config Json`:

```typescript
case 'GOOGLE_REVIEWS':
  if (!config?.placeId || !config?.googleApiKey) {
    throw new AppError('placeId et googleApiKey requis', 400);
  }
```

**Risque**: Si la DB est compromise, toutes les clÃ©s API sont exposÃ©es.

### 6.4 Exposition API

| Endpoint | Protection | Status |
|----------|------------|--------|
| POST /sources/:id/scrape | requireOwnership | âœ… OK |
| GET /sources | requireAuth | âœ… OK |
| POST /sources/analyze | requireAuth | âœ… OK |

### 6.5 Gestion des Secrets

**ProblÃ¨mes**:
- âŒ Credentials en base de donnÃ©es (non chiffrÃ©s)
- âŒ Pas de vault (HashiCorp, AWS Secrets Manager)
- âŒ Logs possiblement exposÃ©s ([`sources.service.ts:215`](api/src/modules/sources/sources.service.ts:215))

---

## 7ï¸âƒ£ OBSERVABILITÃ‰

### 7.1 Logs Existants

| Component | Logs | QualitÃ© |
|-----------|------|---------|
| ScrapingWorker | âœ… JSON structurÃ© | Bonne |
| ScrapingProcessor | âœ… Avec contexte | Bonne |
| SourceAnalyzer | âœ… Avec DiagnosticLog | Excellente |
| BullMQ | âœ… Events | Bonne |

**Exemple de log** ([`scraping.processor.ts:45`](api/src/workers/processors/scraping.processor.ts:45)):
```typescript
logger.info(`Starting scraping for source ${sourceId} (${type})`);
```

### 7.2 Monitoring

**ImplÃ©mentÃ©**:
- âœ… Prometheus metrics (gÃ©nÃ©ral)
- âŒ MÃ©triques scraping spÃ©cifiques manquantes
  - Temps moyen de scraping par source
  - Taux de succÃ¨s par source
  - Erreurs par type

### 7.3 Alerting

**Status**: âŒ **NON IMPLÃ‰MENTÃ‰**

Pas d'alertes pour:
- Source en erreur frÃ©quente
- Rate limit atteint
- Jobs bloquÃ©s
- DonnÃ©es manquantes

### 7.4 TraÃ§abilitÃ© des Jobs

| MÃ©trique | Status |
|----------|--------|
| Job ID | âœ… BullMQ |
| Progress | âœ… job.updateProgress() |
| Status | âœ… ScrapingJob model |
| Duration | âŒ Non calculÃ© explicitement |

### 7.5 DÃ©tection Automatique des Ã‰checs

**Status**: âš ï¸ **PARTIEL**

- âœ… errorCount incrÃ©mentÃ©
- âœ… lastError stockÃ©
- âŒ Pas de notification admin automatique
- âŒ Pas de dÃ©sactivation automatique aprÃ¨s N erreurs

---

## 8ï¸âƒ£ INTÃ‰GRATION AVEC API & FRONTEND

### 8.1 Flux des DonnÃ©es ScrapÃ©es

```
ScrapingWorker
    â”‚
    â”œâ”€â”€ mentionsService.createFromScraper()
    â”‚       â”‚
    â”‚       â””â”€â”€ INSERT into mentions table
    â”‚
    â””â”€â”€ analyticsService.invalidateCache()
            â”‚
            â””â”€â”€ Cache Redis invalidÃ©
                    
Frontend (polling / websocket)
    â”‚
    â”œâ”€â”€ GET /mentions?brandId=...
    â””â”€â”€ GET /analytics/summary?brandId=...
```

### 8.2 DÃ©lais de Propagation

| Ã‰tape | DÃ©lai |
|-------|-------|
| Scraping (terminÃ©) | 0s |
| Insert DB | ~100ms |
| Cache invalidation | ~50ms |
| Propagation frontend | Polling (5-30s) ou WebSocket (instant) |

### 8.3 Risques de DonnÃ©es ObsolÃ¨tes

| ScÃ©nario | Risk |
|----------|------|
| Pollingé—´éš”é•¿ | DonnÃ©es Delayed |
| Cache pas invalidÃ©e | DonnÃ©es stale |
| Scraping en erreur | Pas de mise Ã  jour |

### 8.4 CohÃ©rence Affichage vs Base

**Points de synchronisation**:
- âœ… Cache invalidation explicite aprÃ¨s crÃ©ation
- âœ… Transactions partielles (create + invalidate)
- âš ï¸ Pas de locking lecture/Ã©criture

---

## 9ï¸âƒ£ ANALYSE DES RISQUES CRITIQUES

### Bugs Potentiels

| Bug | ProbabilitÃ© | Impact | GravitÃ© |
|-----|-------------|--------|---------|
| **Job orphelin si worker crash** | Haute | Perte de jobs | CRITIQUE |
| **Mention dupliquÃ©e si retry** | Moyenne | IncohÃ©rence donnÃ©es | MAJEUR |
| **Config exposÃ©e dans logs** | Haute | Fuite credentials | CRITIQUE |
| **Infinite loop sur retry** | Faune | Ressource Ã©puisÃ©e | MAJEUR |
| **Race condition sur errorCount** | Faune | MÃ©triques incorrectes | MINEUR |

### Perte de DonnÃ©es Possible

| ScÃ©nario | Risque |
|----------|--------|
| Scraping API timeout | DonnÃ©es non collectÃ©es |
| DB connection lost en cours | Job perdu |
| Worker restart | Jobs in-progress perdus |
| Rate limit atteint | DonnÃ©es ignorÃ©es |

### Race Conditions

**IdentifiÃ©es**:
1. [`sources.repository.ts:87-92`](api/src/modules/sources/sources.repository.ts:87): `findPendingSources()` sans transaction
2. [`scraping.processor.ts:231`](api/src/workers/processors/scraping.processor.ts:231): Insert sÃ©quentiel avec possibility d'interruption

### Jobs BloquÃ©s

**Causes possibles**:
- Scraping externe timeout (300s)
- Worker crash
- Redis unavailable
- DB deadlock

---

## ğŸ”Ÿ SCORE GLOBAL

| CatÃ©gorie | Score /10 | Justification |
|-----------|-----------|---------------|
| **Architecture** | 7.5 | Structure modulaire, mais couplage fort Worker-Processor |
| **Robustesse** | 5.5 | Pas de circuit breaker, retry basique |
| **ScalabilitÃ©** | 6.0 | Workers fixes, pas de scaling Ã©lastique |
| **FiabilitÃ©** | 6.5 | DÃ©duplication OK, mais risque de perte |
| **MaintenabilitÃ©** | 7.0 | Code propre, mais debugging difficile |
| **SÃ©curitÃ©** | 5.0 | Credentials en plaintext, exposition possible |
| **ObservabilitÃ©** | 6.0 | Logs OK, mais pas d'alerting |

### **SCORE MOYEN: 6.2/10**

---

## 1ï¸âƒ£1ï¸âƒ£ RECOMMANDATIONS PRIORISÃ‰ES

### Quick Wins (ROI Ã‰levÃ©)

| # | Action | ROI | ComplexitÃ© |
|---|--------|-----|------------|
| 1 | **Chiffrer les credentials** (AES-256) | ğŸ”´ Critique | Faible |
| 2 | **Ajouter circuit breaker** | ğŸ”´ Critique | Moyenne |
| 3 | **ImplÃ©menter alerting erreurs** | ğŸŸ  Ã‰levÃ© | Faible |
| 4 | **Corriger logs exposure** | ğŸ”´ Critique | Faible |
| 5 | **AmÃ©liorer retry avec jitter** | ğŸŸ  Ã‰levÃ© | Faible |

### AmÃ©liorations Architecture

| # | Action | ROI | ComplexitÃ© |
|---|--------|-----|------------|
| 1 | **Externaliser secrets** (Vault) | ğŸ”´ Critique | Moyenne |
| 2 | **Workers Ã©lastiques** (K8s) | ğŸŸ  Ã‰levÃ© | Haute |
| 3 | **ImplÃ©menter scheduler fiable** | ğŸŸ¡ Moyen | Moyenne |
| 4 | **Gestion proxy rotation** | ğŸŸ¡ Moyen | Haute |
| 5 | **Versioning mentions** | ğŸŸ¡ Moyen | Moyenne |

### Refactorisations NÃ©cessaires

| # | Action | Rationale |
|---|--------|-----------|
| 1 | **SÃ©parer ScrapingProcessor** du worker | Tester indÃ©pendamment |
| 2 | **Interface pour scrapers** | Ajouter nouveaux scrapers facilement |
| 3 | **Job persistence** | Ã‰viter perte sur crash |
| 4 | **MÃ©trics dÃ©diÃ©es scraping** | Monitoring fin |

---

## 1ï¸âƒ£2ï¸âƒ£ VERSION ARCHITECTURE IDÃ‰ALE (10/10)

### Composants RecommandÃ©s

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SCRAPING ARCHITECTURE 10/10                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ORCHESTRATION LAYER (Kubernetes + Operators)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚Job Controllerâ”‚  â”‚Cron Operator â”‚  â”‚ Auto-scaler        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                    â”‚                    â”‚
        â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Worker Set A â”‚   â”‚ Worker Set B â”‚   â”‚ Worker Set C â”‚
â”‚ (Google)      â”‚   â”‚ (Social)     â”‚   â”‚ (News)       â”‚
â”‚ Proxy Pool    â”‚   â”‚ Proxy Pool   â”‚   â”‚ Proxy Pool   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                    â”‚                    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DATA LAYER                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ PostgreSQL   â”‚  â”‚ Redis (Queue)â”‚  â”‚ Vault (Secrets)   â”‚    â”‚
â”‚  â”‚ + TimescaleDBâ”‚  â”‚ + Streams    â”‚  â”‚ + Rotation        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technologies RecommandÃ©es

| Composant | Technology | Raison |
|-----------|------------|--------|
| **Orchestration** | Kubernetes + Argo Workflows | Ã‰lasticitÃ©, resilience |
| **Queue** | Redis Streams + BullMQ | Persistence, streams |
| **Secrets** | HashiCorp Vault | Rotation automatique |
| **Proxy** | Bright Data / ScraperAPI | Rotation gÃ©rÃ©e |
| **Monitoring** | Grafana + Loki + Prometheus | Full observability |
| **DB** | PostgreSQL + TimescaleDB | Time-series + relations |
| **Alerting** | PagerDuty / OpsGenie | On-call integration |

### Pipeline Optimal

```
1. SCHEDULER
   â””â”€â”€ Kubernetes CronJob (precision second)
       â”‚
       â–¼
2. ORCHESTRATOR
   â””â”€â”€ Argo Workflows (DAG)
       â”‚
       â”œâ”€â”€â–º Proxy Manager (rotation)
       â”‚
       â”œâ”€â”€â–º Scraper Pod (ephemeral)
       â”‚     â””â”€â”€ Timeout: 5min max
       â”‚
       â–¼
3. DATA COLLECTION
   â””â”€â”€ Pub/Sub (Redis Streams)
       â”‚
       â–¼
4. PROCESSING
   â””â”€â”€ Consumer Groups (scalable)
       â”‚
       â”œâ”€â”€â–º Deduplication (Redis)
       â”‚
       â”œâ”€â”€â–º Enrichissement (IA)
       â”‚
       â–¼
5. STORAGE
   â””â”€â”€ PostgreSQL + TimescaleDB
       â”‚
       â–¼
6. API + FRONTEND
   â””â”€â”€ GraphQL + Subscriptions
```

### Patterns RecommandÃ©s

| Pattern | Application |
|---------|-------------|
| **Circuit Breaker** | Toutes les appels externes |
| **Bulkhead** | Pool de connections par source |
| **Dead Letter Queue** | Jobs Ã©chouÃ©s |
| **Event Sourcing** | TraÃ§abilitÃ© complÃ¨te |
| **Idempotency Keys** | Retry safe |
| **Feature Flags** | Routing sources |

---

## ğŸ“Š CONCLUSION

Le systÃ¨me de scraping de Sentinelle Reputation est **fonctionnel** mais prÃ©sente des **lacunes critiques** en termes de:

1. **SÃ©curitÃ©**: Credentials stockÃ©s en plaintext
2. **RÃ©silience**: Pas de circuit breaker, retry basique
3. **ObservabilitÃ©**: Pas d'alerting, mÃ©triques incomplÃ¨tes

**Score actuel: 6.2/10**

En appliquant les corrections proposÃ©es (spÃ©cialement le chiffrement des credentials et l'ajout d'un circuit breaker), le systÃ¨me pourra atteindre **8/10**.

Pour atteindre le **10/10**, une refonte vers l'architecture idÃ©ale proposÃ©e serait nÃ©cessaire.

---

*Rapport gÃ©nÃ©rÃ© le 2026-02-18*
*Audit conduit selon la mÃ©thodologie "Technical Audit Framework"*
